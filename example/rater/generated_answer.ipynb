{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Compare Answers to Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM) compared to a pre-fomulated answer? In this example, we demonstrate how to use AutoRater for verifying the correctness of a generated answers compared to the grounding answer in relation to given question and context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForGeneratedAnswerOpenAIGPT4Config,\n",
    "    RaterForGeneratedAnswerOpenAIGPT3p5Config\n",
    ")\n",
    "from uniflow.op.prompt import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three sample raw inputs. Each one is a tuple consisting of context, question, ground truth answer and generated answer to be labeled. Then we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can only post text on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `prompt_template` in `RaterForGeneratedAnswerOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"), the server (\"OpenAIModelServer\"), number of calls (1), temperature (0), and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"accept\": 1.0, \"equivalent\": 0.0, \"reject\": -1.0}.\n",
    "- `prompt_template` (PromptTemplate): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, grounding answer, generated answer, label, and explanation for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['reject', 'equivalent'] not in example label.\n",
      "RaterForGeneratedAnswerOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                        model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                       model_server='OpenAIModelServer',\n",
      "                                                                       num_call=1,\n",
      "                                                                       temperature=0,\n",
      "                                                                       response_format={'type': 'text'}),\n",
      "                                        label2score={'accept': 1.0,\n",
      "                                                     'equivalent': 0.0,\n",
      "                                                     'reject': -1.0},\n",
      "                                        prompt_template=PromptTemplate(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators and asigned a score of 1.0.', label='accept')]),\n",
      "                                        num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "config.model_config.num_call = 1\n",
    "config.model_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes.\n",
    "\n",
    "NOTE: The printed information `\"The label2score label ['reject', 'equivalent'] not in example label.\"` is because we only pass one example (label=`accept`) in default `prompt_template` to reduce token consumption when using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['reject', 'equivalent'] not in example label.\n",
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.0, 'response_format': {'type': 'json_object'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, prompt_template=PromptTemplate(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators and asigned a score of 1.0.', label='accept')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then, we can run the client. For each item in the raw input, the Client will generate an explanation and a final label in [`Accept`, `Equivalent`, `Reject`]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': [{'explanation': 'The grounding answer is incorrect '\n",
      "                                           'as it states that users can only '\n",
      "                                           'post text on Reddit, which '\n",
      "                                           'contradicts the context provided. '\n",
      "                                           'The generated answer correctly '\n",
      "                                           'includes the variety of content '\n",
      "                                           'types that can be submitted on '\n",
      "                                           'Reddit, such as links, text posts, '\n",
      "                                           'images, and videos, which is in '\n",
      "                                           'line with the context. Therefore, '\n",
      "                                           'the generated answer is better.',\n",
      "                            'label': 'accept'}],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fedef6a3c40>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': [{'explanation': 'The generated answer is less '\n",
      "                                           'precise than the grounding answer. '\n",
      "                                           'The grounding answer provides the '\n",
      "                                           'exact release year (2009) for '\n",
      "                                           'League of Legends, while the '\n",
      "                                           'generated answer vaguely states '\n",
      "                                           \"'the early 2000s,' which could \"\n",
      "                                           'imply any year from 2000 to 2005 '\n",
      "                                           'and is not specific. Therefore, '\n",
      "                                           'the grounding answer is more '\n",
      "                                           'accurate and informative in '\n",
      "                                           'response to the question.',\n",
      "                            'label': 'reject'}],\n",
      "              'scores': [-1.0],\n",
      "              'votes': ['reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fedef6a38e0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': [{'explanation': 'The generated answer is equivalent '\n",
      "                                           'to the grounding answer as both '\n",
      "                                           'correctly identify that Vitamin C '\n",
      "                                           'is water-soluble. The phrasing is '\n",
      "                                           'slightly different but the meaning '\n",
      "                                           'is the same, so the generated '\n",
      "                                           'answer is just as correct as the '\n",
      "                                           'grounding answer.',\n",
      "                            'label': 'equivalent'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fedef6a3cd0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'The grounding answer is incorrect as it states that users can '\n",
      "                'only post text on Reddit, which contradicts the context '\n",
      "                'provided. The generated answer correctly includes the variety '\n",
      "                'of content types that can be submitted on Reddit, such as '\n",
      "                'links, text posts, images, and videos, which is in line with '\n",
      "                'the context. Therefore, the generated answer is better.',\n",
      " 'label': 'accept'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only sample LLM once so the majority vote is the only label for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31maccept\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mreject\u001b[0m and score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has label \u001b[31mequivalent\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, prompt_template=PromptTemplate(instruction=\"\\n            # Task: Evaluate and compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            ## Input: A sample to be labeled:\\n            1. context: A brief text containing key information.\\n            2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n            3. grounding Answer: Pre-formulated, usually from human.\\n            4. generated Answer: From a language model.\\n            ## Evaluation Criteria: If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            ## Response Format: Your response should only include two fields below:\\n            1. explanatoin: Reasoning behind your judgment, detailing why the generated answer is better, equivalent or worse.\\n            2. label: Your judgment (one of ['accept', 'equivalent', 'reject']).\\n            ## Note:\\n            Only use the example below as a few shot demonstrate but not include them in the final response. Your response should only focus on the unlabeled sample.\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators and asigned a score of 1.0.', label='accept'), Context(context='Operating systems(OS) did not exist in their modern and more complex forms until the early 1960s.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='The generated answer is as equally good as grounding answer because they both accurately pinpoint the early 1960s as the period when modern operating systems began to develop and asigned a score of 0.0.', label='equivalent'), Context(context='Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing in the 1960s.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context and asigned a score of -1.0.', label='reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForGeneratedAnswerOpenAIGPT3p5Config()\n",
    "config2.model_config.num_call = 3\n",
    "config2.model_config.temperature = 0.9\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the label is determined by taking the majority vote from three samples of the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': ['explanation: The generated answer is better '\n",
      "                           'because it accurately identifies the various types '\n",
      "                           'of content that users can submit on Reddit, which '\n",
      "                           'aligns with the information provided in the '\n",
      "                           'context. The grounding answer is too restrictive '\n",
      "                           'and does not fully capture the range of content '\n",
      "                           'that can be posted on Reddit, so the generated '\n",
      "                           'answer is more comprehensive, earning a score of '\n",
      "                           '1.0.\\n'\n",
      "                           'label: accept',\n",
      "                           'explanation: The generated answer is better '\n",
      "                           'because it accurately identifies the various types '\n",
      "                           'of content that users can submit on Reddit, '\n",
      "                           'including links, text posts, images, and videos, '\n",
      "                           'which is consistent with the context provided. The '\n",
      "                           'grounding answer inaccurately states that users '\n",
      "                           'can only post text on Reddit, which is '\n",
      "                           'contradicted by the context. Therefore, the '\n",
      "                           'generated answer is more comprehensive and '\n",
      "                           'accurate, earning a score of 1.0.\\n'\n",
      "                           'label: accept',\n",
      "                           'explanation: The generated answer is better '\n",
      "                           'because it accurately identifies the various types '\n",
      "                           'of content that users can submit on Reddit, as '\n",
      "                           'mentioned in the context. The grounding answer is '\n",
      "                           'not correct as it states that users can only post '\n",
      "                           'text on Reddit, which is not true based on the '\n",
      "                           'given information. Therefore, the generated answer '\n",
      "                           'is superior and is assigned a score of 1.0.\\n'\n",
      "                           'label: accept'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['accept', 'accept', 'accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fedef7608e0>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': ['explanation: The generated answer is worse because '\n",
      "                           'it inaccurately states the release date of League '\n",
      "                           'of Legends as the early 2000s instead of 2009, as '\n",
      "                           'mentioned in the context. This is a significant '\n",
      "                           'factual error, resulting in a score of -1.0.\\n'\n",
      "                           'label: reject',\n",
      "                           'explanation: The generated answer is worse because '\n",
      "                           'it inaccurately states the release year of League '\n",
      "                           'of Legends as the early 2000s, which is not '\n",
      "                           'supported by the context. The correct release '\n",
      "                           'year, as provided in the context, is 2009, so the '\n",
      "                           'generated answer is incorrect and receives a score '\n",
      "                           'of -1.0.\\n'\n",
      "                           'label: reject',\n",
      "                           'explanation: The generated answer is worse because '\n",
      "                           'it inaccurately states the release time of League '\n",
      "                           'of Legends as the early 2000s instead of the '\n",
      "                           'correct year 2009 and asigned a score of -1.0.\\n'\n",
      "                           'label: reject'],\n",
      "              'scores': [-1.0, -1.0, -1.0],\n",
      "              'votes': ['reject', 'reject', 'reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fee100b76d0>},\n",
      " {'output': [{'average_score': -0.3333333333333333,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': ['explanation: The generated answer is equivalent to '\n",
      "                           'the grounding answer because they both correctly '\n",
      "                           'identify Vitamin C as a water-soluble vitamin, and '\n",
      "                           'asigned a score of 0.0.\\n'\n",
      "                           'label: equivalent',\n",
      "                           'explanation: The generated answer is equivalent '\n",
      "                           'because it accurately confirms that Vitamin C is '\n",
      "                           'water-soluble and asigned a score of 0.0.\\n'\n",
      "                           'label: equivalent',\n",
      "                           'explanation: The grounding answer is better '\n",
      "                           'because it directly states that Vitamin C is a '\n",
      "                           '\"very water-soluble vitamin,\" whereas the '\n",
      "                           'generated answer only implies that it can be '\n",
      "                           'dissolved in water. The grounding answer provides '\n",
      "                           'a more definitive and accurate statement about the '\n",
      "                           'water solubility of Vitamin C and is thus '\n",
      "                           'superior, with a score of 1.0.\\n'\n",
      "                           'label: reject'],\n",
      "              'scores': [0.0, 0.0, -1.0],\n",
      "              'votes': ['equivalent', 'equivalent', 'reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fedef7621a0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from multiple LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31maccept\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mreject\u001b[0m and average score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mequivalent\u001b[0m and average score \u001b[34m-0.3333333333333333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
