{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate self-instruct dataset for d2lai book\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "[In this section, we present three scenarios for illustration](#number-qas):\n",
    "\n",
    "1. A single question and its corresponding answer.\n",
    "2. A set of three questions, each with its own answer.\n",
    "3. A group of five questions, again each with a specific answer.\n",
    "\n",
    "Chapter 22 - Mathematics for Deep Learning\n",
    "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/envs/uniflow/lib/python3.10/site-packages (0.0.348)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/uniflow/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/uniflow/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/envs/uniflow/lib/python3.10/site-packages (0.41.3.post2)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/uniflow/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.1,>=0.0.12 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (0.0.12)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (0.0.69)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from accelerate) (2.2.0.dev20231209+cu121)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from langchain-core<0.1,>=0.0.12->langchain) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0rc1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: pytorch-triton==2.1.0+bcad9dabe1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0+bcad9dabe1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/uniflow/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install langchain transformers accelerate bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformHuggingFaceConfig, HuggingfaceModelConfig\n",
    "from uniflow.op.prompt_schema import GuidedPrompt, Context\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "First, we need to pre-process the HTML to get text chunks that we can feed into the model. We will use `UnstructuredHTMLLoader` from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file = \"22.11_information-theory.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set current directory and input data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(input_file)\n",
    "pages = loader.load_and_split()\n",
    "page_contents = [page.page_content for page in pages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM, those include instruction and sample json format. We do this by giving a sample instruction and list of `Context` examples to the `GuidedPrompt` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = GuidedPrompt(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    examples=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context='Before we get started, let’s outline the relationship between machine\\nlearning and information theory. Machine learning aims to extract\\ninteresting signals from data and make critical predictions. On the\\nother hand, information theory studies encoding, decoding, transmitting,\\nand manipulating information. As a result, information theory provides\\nfundamental language for discussing the information processing in\\nmachine learned systems. For example, many machine learning applications\\nuse the cross-entropy loss as described in Section 4.1. This\\nloss can be directly derived from information theoretic considerations.'),\n",
       " Context(context='Let’s start with the “soul” of information theory: information.\\nInformation can be encoded in anything with a particular sequence of\\none or more encoding formats. Suppose that we task ourselves with trying\\nto define a notion of information. What could be our starting point?'),\n",
       " Context(context='Consider the following thought experiment. We have a friend with a deck\\nof cards. They will shuffle the deck, flip over some cards, and tell us\\nstatements about the cards. We will try to assess the information\\ncontent of each statement.'),\n",
       " Context(context='Next, they flip over a card and say, “I see a heart.” This provides us\\nsome information, but in reality there are only \\\\(4\\\\) different\\nsuits that were possible, each equally likely, so we are not surprised\\nby this outcome. We hope that whatever the measure of information, this\\nevent should have low information content.'),\n",
       " Context(context='Next, they flip over a card and say, “This is the \\\\(3\\\\) of spades.”\\nThis is more information. Indeed there were \\\\(52\\\\) equally likely\\npossible outcomes, and our friend told us which one it was. This should\\nbe a medium amount of information.'),\n",
       " Context(context='Let’s take this to the logical extreme. Suppose that finally they flip\\nover every card from the deck and read off the entire sequence of the\\nshuffled deck. There are \\\\(52!\\\\) different orders to the deck, again\\nall equally likely, so we need a lot of information to know which one it\\nis.'),\n",
       " Context(context='Any notion of information we develop must conform to this intuition.\\nIndeed, in the next sections we will learn how to compute that these\\nevents have \\\\(0\\\\textrm{ bits}\\\\), \\\\(2\\\\textrm{ bits}\\\\),\\n\\\\(~5.7\\\\textrm{ bits}\\\\), and \\\\(~225.6\\\\textrm{ bits}\\\\) of\\ninformation respectively.'),\n",
       " Context(context='If we read through these thought experiments, we see a natural idea. As\\na starting point, rather than caring about the knowledge, we may build\\noff the idea that information represents the degree of surprise or the\\nabstract possibility of the event. For example, if we want to describe\\nan unusual event, we need a lot information. For a common event, we may\\nnot need much information.'),\n",
       " Context(context='In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = [ Context(context=p) for p in pages[2].page_content.split(\"\\n\\n\") if len(p) > 200]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for the given `page_contents` above, we convert them to the `Context` class to be processed by `uniflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "transform_config = TransformHuggingFaceConfig(\n",
    "    guided_prompt_template=guided_prompt,\n",
    "    model_config=HuggingfaceModelConfig(batch_size=128)\n",
    ")\n",
    "client = TransformClient(transform_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:08<00:00, 188.53s/it]\n"
     ]
    }
   ],
   "source": [
    "output = client.run(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the output\n",
    "\n",
    "Let's take a look of the generated output. We need to do a little postprocessing on the raw output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Generate one question and its corresponding answer based on the last context in the last\n",
      "    example. Follow the format of the examples below to include context, question, and answer in the response\n",
      "context: In 1948, Claude E. Shannon published A Mathematical Theory of\n",
      "Communication (Shannon, 1948) establishing the theory of\n",
      "information. In his article, Shannon introduced the concept of\n",
      "information entropy for the first time. We will begin our journey here.\n",
      "question: Who published A Mathematical Theory of Communication in 1948?\n",
      "answer: Claude E. Shannon.\n",
      "context: Before we get started, let’s outline the relationship between machine\n",
      "learning and information theory. Machine learning aims to extract\n",
      "interesting signals from data and make critical predictions. On the\n",
      "other hand, information theory studies encoding, decoding, transmitting,\n",
      "and manipulating information. As a result, information theory provides\n",
      "fundamental language for discussing the information processing in\n",
      "machine learned systems. For example, many machine learning applications\n",
      "use the cross-entropy loss as described in Section 4.1. This\n",
      "loss can be directly derived from information theoretic considerations.\n",
      "question: How does information theory relate to machine learning?\n",
      "answer: Information theory provides fundamental language for discussing the information processing in machine learned systems. Many machine learning applications use the cross-entropy loss which is directly derived from information theoretic considerations.\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['output'][0]['response'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ca45c th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ca45c_row0_col0, #T_ca45c_row0_col1, #T_ca45c_row0_col2, #T_ca45c_row1_col0, #T_ca45c_row1_col1, #T_ca45c_row1_col2, #T_ca45c_row2_col0, #T_ca45c_row2_col1, #T_ca45c_row2_col2, #T_ca45c_row3_col0, #T_ca45c_row3_col1, #T_ca45c_row3_col2, #T_ca45c_row4_col0, #T_ca45c_row4_col1, #T_ca45c_row4_col2, #T_ca45c_row5_col0, #T_ca45c_row5_col1, #T_ca45c_row5_col2, #T_ca45c_row6_col0, #T_ca45c_row6_col1, #T_ca45c_row6_col2, #T_ca45c_row7_col0, #T_ca45c_row7_col1, #T_ca45c_row7_col2, #T_ca45c_row8_col0, #T_ca45c_row8_col1, #T_ca45c_row8_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ca45c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ca45c_level0_col0\" class=\"col_heading level0 col0\" >Context</th>\n",
       "      <th id=\"T_ca45c_level0_col1\" class=\"col_heading level0 col1\" >Question</th>\n",
       "      <th id=\"T_ca45c_level0_col2\" class=\"col_heading level0 col2\" >Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ca45c_row0_col0\" class=\"data row0 col0\" > Before we get started, let’s outline the relationship between machine\n",
       "learning and information theory. Machine learning aims to extract\n",
       "interesting signals from data and make critical predictions. On the\n",
       "other hand, information theory studies encoding, decoding, transmitting,\n",
       "and manipulating information. As a result, information theory provides\n",
       "fundamental language for discussing the information processing in\n",
       "machine learned systems. For example, many machine learning applications\n",
       "use the cross-entropy loss as described in Section 4.1. This\n",
       "loss can be directly derived from information theoretic considerations.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row0_col1\" class=\"data row0 col1\" > How does information theory relate to machine learning?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row0_col2\" class=\"data row0 col2\" > Information theory provides fundamental language for discussing the information processing in machine learned systems. Many machine learning applications use the cross-entropy loss which is directly derived from information theoretic considerations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ca45c_row1_col0\" class=\"data row1 col0\" > The concept of information entropy was introduced by Claude E. Shannon in his paper titled \"A Mathematical Theory of Communication\" in 1948.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row1_col1\" class=\"data row1 col1\" > When was the concept of information entropy introduced?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row1_col2\" class=\"data row1 col2\" > The concept of information entropy was introduced by Claude E. Shannon in 1948.\n",
       "```\n",
       "\n",
       "## Answer (2)\n",
       "\n",
       "**Context:** In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948), which established the theory of information. In this paper, he introduced the concept of information entropy for the first time.\n",
       "\n",
       "**Question:** What is the name of the person who published A Mathematical Theory of Communication in 1948?\n",
       "\n",
       "**Answer:** Claude E. Shannon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ca45c_row2_col0\" class=\"data row2 col0\" > Consider the following thought experiment. We have a friend with a deck\n",
       "of cards. They will shuffle the deck, flip over some cards, and tell us\n",
       "statements about the cards. We will try to assess the information\n",
       "content of each statement. For example, if they say \"There are two red\n",
       "cards,\" we might think that this statement has less information content than\n",
       "if they said \"The king is red.\" However, it turns out that the second\n",
       "statement actually has more information content because there are only four\n",
       "red cards in the deck. This shows that the information content of a statement depends not just on what it says but also on how much we know already.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row2_col1\" class=\"data row2 col1\" > What does the information content of a statement depend on?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row2_col2\" class=\"data row2 col2\" > The information content of a statement depends on both what it says and how much we already know.\n",
       "```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ca45c_row3_col0\" class=\"data row3 col0\" > Next, they flip over a card and say, “I see a heart.” This provides us\n",
       "some information, but in reality there are only \\(4\\) different\n",
       "suits that were possible, each equally likely, so we are not surprised\n",
       "by this outcome. We hope that whatever the measure of information, this\n",
       "event should have low information content.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row3_col1\" class=\"data row3 col1\" > What is the number of suits that were possible when flipping over a card?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row3_col2\" class=\"data row3 col2\" > There were four suits possible.\n",
       "```\n",
       "\n",
       "## Answer (2)\n",
       "\n",
       "**Question:** Let $X$ be a discrete random variable with probability mass function $p_x$. Suppose $p_x = \\frac{1}{n}$ for all $x$, where $n$ is some positive integer. Is $X$ independent from itself?\n",
       "\n",
       "**Answer:** Yes, $X$ is independent from itself.\n",
       "\n",
       "*Proof:* For any two distinct values $x_1, x_2$ of $X$, $$P(X=x_1, X=x_2) = P(X=x_1)P(X=x_2) = \\left(\\frac{1}{n}\\right)^2 = \\frac{1}{n^2}.$$ Since $\\frac{1}{n^2} > 0$, it follows that $X$ is dependent on itself. However, since $p_x = \\frac{1}{n}$ for all $x$, $X$ has no conditional probabilities, which means that $X$ is conditionally independent given every other random variable. Thus, $X$ is unconditionally independent from itself.\n",
       "\n",
       "Comment: I think you meant \"dependent\" instead of \"independent\". The proof is correct though.\n",
       "\n",
       "Comment: @MichaelHardy You're right; thanks! I fixed my mistake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ca45c_row4_col0\" class=\"data row4 col0\" > The next step is to calculate the probability of each outcome. For instance, if we know that the probability of getting heads is 0.5, then the probability of getting tails is also 0.5.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row4_col1\" class=\"data row4 col1\" > What is the probability of getting tails when flipping over a fair coin?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row4_col2\" class=\"data row4 col2\" > The probability of getting tails when flipping over a fair coin is 0.5.\n",
       "```\n",
       "\n",
       "## Answer (6)\n",
       "\n",
       "**Question:** Let $p$ be the probability of picking a red marble from an urn containing only red marbles. Suppose that you pick two marbles without replacement. What is the probability of both marbles being red?\n",
       "\n",
       "**Answer:** $\\frac{p}{1-p}$\n",
       "\n",
       "--------------------\n",
       "\n",
       "The probability of picking a red marble on your first draw is $p$. After removing one red marble, there remain $p-1$ red marbles left in the urn. So the probability of picking another red marble on your second draw is $(p-1)/(1-p)$. Since these events happen independently, their probabilities multiply together. Therefore, the overall probability of drawing two red marbles consecutively is $$\\frac{(p-1)}{(1-p)}=\\frac{p}{1-p}.$$\n",
       "\n",
       "Comment: I think this is correct but not complete. You need to explain why the events happening consecutively have independent probabilities.\n",
       "\n",
       "Comment: @MichaelHardy Yes, you're right. Thank you for pointing out my mistake!\n",
       "\n",
       "Comment: @JamesK.Polk No problem at all! If you have any other questions or doubts about anything else, feel free to ask! :)\n",
       "\n",
       "Comment: @JamesK.Polk I added some explanation now. Hopefully that helps!\n",
       "\n",
       "Comment: @JamesK.Polk Great job! Your solution looks good too. It's always nice to see different approaches to solving problems. Keep up the good work! :)\n",
       "\n",
       "Comment: @JamesK.Polk Thanks! I appreciate your kind words. Have a great day! :)\n",
       "\n",
       "Comment: @JamesK.Polk I just realized something interesting. If we let $q=1-p$, then the probability of picking two red marbles consecutively can be written as $$ \\frac{p}{1-p}=\\frac{p}{(1-p)^2} = \\frac{p}{q^2} = \\frac{1}{q}. $$ Thus, the probability of picking two red marbles consecutively is equal to the reciprocal of the probability of picking a non-red marble on the first draw. That's pretty cool!\n",
       "\n",
       "Comment: @JamesK.Polk Yeah, that's really neat! Do you want me to add that observation to my answer?\n",
       "\n",
       "Comment: @JamesK.Polk Sure thing! I updated my answer with that observation. Hopefully that makes things clearer!\n",
       "\n",
       "Comment: @JamesK.Polk Awesome! I hope you find it helpful. Let me know if you have any further questions or concerns. Good luck with everything! :)\n",
       "\n",
       "Comment: @JamesK.Polk I just noticed something else interesting. If we let $r$ be the number of red marbles in the urn initially, then the probability of picking two red marbles consecutively can be expressed as $$ \\frac{pr}{(r+1)(r+2)} + \\</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_ca45c_row5_col0\" class=\"data row5 col0\" > Let’s take this to the logical extreme. Suppose that finally they flip\n",
       "over every card from the deck and read off the entire sequence of the\n",
       "shuffled deck. There are \\(52!\\) different orders to the deck, again\n",
       "all equally likely, so we need a lot of information to know which one it\n",
       "is. This is known as the “infinite information” problem.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row5_col1\" class=\"data row5 col1\" > What is the infinite information problem?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row5_col2\" class=\"data row5 col2\" > The infinite information problem refers to the situation where there are an infinite number of possible outcomes or sequences, all equally likely, and therefore require an infinite amount of information to determine which one has occurred.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_ca45c_row6_col0\" class=\"data row6 col0\" > Any notion of information we develop must conform to this intuition.\n",
       "Indeed, in the next sections we will learn how to compute that these\n",
       "events have \\(0\\textrm{ bits}\\), \\(2\\textrm{ bits}\\),\n",
       "\\(~5.7\\textrm{ bits}\\), and \\(~225.6\\textrm{ bits}\\) of\n",
       "information respectively.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row6_col1\" class=\"data row6 col1\" > What is the number of bits required to represent an event with zero bits of information?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row6_col2\" class=\"data row6 col2\" > The number of bits required to represent an event with zero bits of information is zero.\n",
       "```\n",
       "\n",
       "## Answer (3)\n",
       "\n",
       "The number of bits required to represent an event with zero bits of information is zero.\n",
       "\n",
       "Comment: This is a good answer but it would be better if you could provide some explanation or reference to support your claim. For instance, why does the author say \"Any notion of information we develop must conform to this intuition\"? Is there any mathematical definition of information entropy that requires this property?\n",
       "\n",
       "Comment: @user1234567 I don't think there is anything special about events with zero bits of information. It just means they are completely predictable and can be represented by a single bit. If you want more detail, look up [entropy](https://en.wikipedia.org/wiki/Entropy_(statistical_mechanics)) and [information theory](https://en.wikipedia.org/wiki/Information_theory).\n",
       "\n",
       "Comment: @user1234567 You may also find [this paper](http://www.csse.monash.edu.au/~lloyd/Papers/Info-Theory.pdf) helpful.\n",
       "\n",
       "Comment: @user1234567 Also note that the statement \"any notion of information\" refers to the idea of using information as a measure of uncertainty, which is not always the case. For example, in cryptography, information is often used to mean something else entirely.\n",
       "\n",
       "Comment: @user1234567 I agree with all of those points. However, I still believe that the statement \"any notion of information\" implies that the author has some sort of mathematical definition of information entropy that requires this property. Otherwise, what is the point of saying \"must conform to this intuition\"?\n",
       "\n",
       "Comment: @user1234567 I disagree. There is no requirement that information entropy should satisfy this property. Information entropy is simply a way of measuring the amount of uncertainty associated with a random variable. Whether or not it satisfies this particular property depends on the specific application.\n",
       "\n",
       "Comment: @user1234567 As far as I know, there is no mathematical definition of information entropy that requires this property. Instead, it is defined as the average logarithm of the probability distribution over possible outcomes. So if you have a system where every outcome is certain, then the probability distribution is trivial and the entropy is zero. That doesn't make sense unless you assume that the entropy measures uncertainty rather than information.\n",
       "\n",
       "Comment: @user1234567 I see your point now. Thank you for clarifying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_ca45c_row7_col0\" class=\"data row7 col0\" > If we read through these thought experiments, we see a natural idea. As\n",
       "a starting point, rather than caring about the knowledge, we may build\n",
       "off the idea that information represents the degree of surprise or the\n",
       "abstract possibility of the event. For example, if we want to describe\n",
       "an unusual event, we need a lot information. For a common event, we may\n",
       "not need much information. This is because the amount of information needed\n",
       "to describe an event depends on how surprising it is. The more surprising\n",
       "the event, the more information we need to describe it.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row7_col1\" class=\"data row7 col1\" > What does information represent according to this idea?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row7_col2\" class=\"data row7 col2\" > Information represents the degree of surprise or the abstract possibility of the event.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca45c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_ca45c_row8_col0\" class=\"data row8 col0\" > In 1948, Claude E. Shannon published A Mathematical Theory of\n",
       "Communication (Shannon, 1948) establishing the theory of\n",
       "information. In his article, Shannon introduced the concept of\n",
       "information entropy for the first time. We will begin our journey here.\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row8_col1\" class=\"data row8 col1\" > What was the title of the paper that Claude E. Shannon published in 1948?\n",
       "</td>\n",
       "      <td id=\"T_ca45c_row8_col2\" class=\"data row8 col2\" > The title of the paper was \"A Mathematical Theory of Communication\".</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb6c0e63b20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting context, question, and answer into a DataFrame\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "keywords = [\"context:\", \"question:\", \"answer:\"]\n",
    "pattern = '|'.join(map(re.escape, keywords))\n",
    "\n",
    "for item in output[0]['output'][0]['response']:\n",
    "    segments = [segment for segment in re.split(pattern, item) if segment.strip()]\n",
    "\n",
    "    contexts.append(segments[-3])\n",
    "    questions.append(segments[-2])\n",
    "    answers.append(segments[-1])\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Context': contexts,\n",
    "    'Question': questions,\n",
    "    'Answer': answers\n",
    "})\n",
    "\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left'}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('text-align', 'left')]\n",
    "}])\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = df[['Question', 'Answer']]\n",
    "\n",
    "output_dir = 'data/output'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_df.to_csv(f\"{output_dir}/selfinstruct_d2lai.csv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
