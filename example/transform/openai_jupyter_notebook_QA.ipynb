{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Generating QAs from a Jupyter Notebook\n",
    "\n",
    "In this example, we will show you how to generate question-answer pairs from a given jupyter notebook.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093be070",
   "metadata": {},
   "source": [
    "### Import dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseortiz/anaconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformOpenAIConfig\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt_schema import Context, GuidedPrompt\n",
    "\n",
    "from langchain.document_loaders import NotebookLoader\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e2d4d",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "First, we need to pre-process the given jupyter notebook `model.ipynb` to get text chunks that we can feed into the model. We will use `NotebookLoader` from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092b355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "jupyter_notebook_file = \"model.ipynb\"\n",
    "input_file = os.path.join(f\"{dir_cur}\", jupyter_notebook_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseortiz/anaconda3/envs/uniflow/lib/python3.10/site-packages/langchain_community/document_loaders/notebook.py:122: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\'markdown\\' cell: \\'[\\'# Notebook for ModelFlow \\', \\'\\', \"In this example, we will show you how to generate question-answers (QAs) from give text strings using OpenAI\\'s models via uniflow\\'s [ModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L11).\", \\'\\', \\'### Before running the code\\', \\'\\', \\'You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\\', \\'\\', \\'Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\\', \\'\\', \\'### Update system path\\']\\'\\n\\n \\'code\\' cell: \\'[\\'%reload_ext autoreload\\', \\'%autoreload 2\\', \\'\\', \\'import sys\\', \\'\\', \\'sys.path.append(\".\")\\', \\'sys.path.append(\"..\")\\', \\'sys.path.append(\"../..\")\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Import dependency\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from dotenv import load_dotenv\\', \\'from IPython.display import display\\', \\'\\', \\'from uniflow.transform.client import Client\\', \\'from uniflow.flow_factory import FlowFactory\\', \\'from uniflow.transform.config import TransformConfig\\', \\'from uniflow.model.config import OpenAIModelConfig\\', \\'from uniflow.viz import Viz\\', \\'from uniflow.schema import Context\\', \\'\\', \\'load_dotenv()\\']\\'\\n with output: \\'[\\'/Users/joseortiz/anaconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\\\n\\', \\'  from .autonotebook import tqdm as notebook_tqdm\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Display the different flows\\']\\'\\n\\n  \\'markdown\\' cell: \\'[\\'### Prepare Sample Prompts\\', \\'Here, we will use the following sample prompts from which to generate QAs.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'raw_context_input = [\\', \\'    \"It was a sunny day and the sky color is blue.\",\\', \\'    \"My name is Bobby and I am a talent software engineer working on AI/ML\",\\', \\']\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Next, for the given raw text strings `raw_context_input` above, we convert them to the `Context` class to be processed by `uniflow`.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'\\', \\'data = [\\', \\'    Context(context=c)\\', \\'    for c in raw_context_input\\', \\']\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Use LLM to generate data\\', \\'In this example, we use the base `Config` defaults with the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) to generate questions and answers.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'config = TransformConfig(\\', \\'    flow_name=\"TransformOpenAIFlow\",\\', \\'    model_config=OpenAIModelConfig()\\', \\')\\', \\'client = Client(config)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'output = client.run(data)\\', \\'output\\']\\'\\n with output: \\'[\\'100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### View the output\\', \\'\\', \"Let\\'s take a look of the generated output.\"]\\'\\n\\n  \\'markdown\\' cell: \\'[\\'### Plot model flow graph\\', \\'Here, we visualize the model flow graph for the `ModelFlow`.\\']\\'\\n\\n \\'code\\' cell: \\'[\"graph = Viz.to_digraph(output[0][\\'root\\'])\"]\\'\\n\\n  \\'code\\' cell: \\'[\"graph = Viz.to_digraph(output[1][\\'root\\'])\"]\\'\\n\\n '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = NotebookLoader(input_file,\n",
    "                        include_outputs=True,\n",
    "                        max_output_length=1000,\n",
    "                        remove_newline=True)\n",
    "raw_content = loader.load()\n",
    "raw_content[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the loaded jupyter notebook content is quite messy. Let's split this content by each markdown header!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_based_on_markdown_header(s, markdown_symbol, code_symbol):\n",
    "    \"\"\"\n",
    "    Splits a string based on a list of tokens including markdown_symbol and code_symbol,\n",
    "    then further process the list so that the given string s is splits based on each markdown header.\n",
    "\n",
    "    :param s: The string to be split.\n",
    "    :param markdown_symbol: A symbol that represented a markdown cell.\n",
    "    :param code_symbol: A symbol that represented a code cell.\n",
    "    :return: A list of strings, split based markdown header.\n",
    "    \"\"\"\n",
    "    # Create a regular expression pattern from the tokens, ensuring to escape special characters\n",
    "    pattern = '|'.join(re.escape(token) for token in [markdown_symbol, code_symbol])\n",
    "\n",
    "    # Use re.split() to split the string, but keep the tokens in the result\n",
    "    strings = re.split(f'(?={pattern})', s)\n",
    "\n",
    "    # Further process the list\n",
    "    processed = []\n",
    "    for s in strings:\n",
    "        # Check if the string starts with \"'markdown' cell: '\"\n",
    "        if s.startswith(markdown_symbol):\n",
    "            # Split the string by the specified pattern ', '\n",
    "            parts = re.split(\"\"\"', '\"\"\" , s)\n",
    "\n",
    "            # Process each part\n",
    "            new_list = []\n",
    "            for part in parts:\n",
    "                # If the part starts with \"#\", \"##\", or \"###\", it remains standalone\n",
    "                if part.startswith(\"#\") or part.startswith(\"##\") or part.startswith(\"###\"):\n",
    "                    new_list.append(part)\n",
    "                # Otherwise, it is appended to the previous part\n",
    "                else:\n",
    "                    if new_list:\n",
    "                        new_list[-1] += \"\\n\"\n",
    "                        new_list[-1] += part\n",
    "                    else:\n",
    "                        # If it's the first part and doesn't start with \"#\", it's added as is\n",
    "                        new_list.append(part)\n",
    "\n",
    "            # Add the processed parts to the main list\n",
    "            processed.extend(new_list)\n",
    "\n",
    "        # For strings starting with \"\"\"'code' cell\"\"\", append them to the last string\n",
    "        elif s.startswith(code_symbol):\n",
    "            processed[-1] += s\n",
    "        else:\n",
    "            # Remove empty lines\n",
    "            if len(s.replace(\"\\n\", \"\")) > 0:\n",
    "                processed.append(s)\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the `split_string_based_on_markdown_header` function, let's split our loaded jupyter notebook! It's much more semanticly organized now as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= New Header ================\n",
      "'markdown' cell: '['# Notebook for ModelFlow \n",
      "', \"In this example, we will show you how to generate question-answers (QAs) from give text strings using OpenAI's models via uniflow's [ModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L11).\", '\n",
      "================= New Header ================\n",
      "### Before running the code\n",
      "\n",
      "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
      "\n",
      "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
      "\n",
      "================= New Header ================\n",
      "### Update system path']'\n",
      "\n",
      " 'code' cell: '['%reload_ext autoreload', '%autoreload 2', '', 'import sys', '', 'sys.path.append(\".\")', 'sys.path.append(\"..\")', 'sys.path.append(\"../..\")']'\n",
      "\n",
      " \n",
      "================= New Header ================\n",
      "'markdown' cell: '['## Import dependency']'\n",
      "\n",
      " 'code' cell: '['from dotenv import load_dotenv', 'from IPython.display import display', '', 'from uniflow.transform.client import Client', 'from uniflow.flow_factory import FlowFactory', 'from uniflow.transform.config import TransformConfig', 'from uniflow.model.config import OpenAIModelConfig', 'from uniflow.viz import Viz', 'from uniflow.schema import Context', '', 'load_dotenv()']'\n",
      " with output: '['/Users/joseortiz/anaconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\n', '  from .autonotebook import tqdm as notebook_tqdm\\n']'\n",
      "\n",
      " \n",
      "================= New Header ================\n",
      "'markdown' cell: '['### Display the different flows']'\n",
      "\n",
      "  \n",
      "================= New Header ================\n",
      "'markdown' cell: '['### Prepare Sample Prompts\n",
      "Here, we will use the following sample prompts from which to generate QAs.']'\n",
      "\n",
      " 'code' cell: '['raw_context_input = [', '    \"It was a sunny day and the sky color is blue.\",', '    \"My name is Bobby and I am a talent software engineer working on AI/ML\",', ']']'\n",
      "\n",
      " \n",
      "================= New Header ================\n",
      "'markdown' cell: '['Next, for the given raw text strings `raw_context_input` above, we convert them to the `Context` class to be processed by `uniflow`.']'\n",
      "\n",
      " 'code' cell: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'\n",
      "\n",
      " \n",
      "================= New Header ================\n",
      "'markdown' cell: '['### Use LLM to generate data\n",
      "In this example, we use the base `Config` defaults with the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) to generate questions and answers.']'\n",
      "\n",
      " 'code' cell: '['config = TransformConfig(', '    flow_name=\"TransformOpenAIFlow\",', '    model_config=OpenAIModelConfig()', ')', 'client = Client(config)']'\n",
      "\n",
      " \n",
      "================= New Header ================\n",
      "'markdown' cell: '['Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above.']'\n",
      "\n",
      " 'code' cell: '['output = client.run(data)', 'output']'\n",
      " with output: '['100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\\n']'\n",
      "\n",
      " \n",
      "================= New Header ================\n",
      "'markdown' cell: '['### View the output\n",
      "', \"Let's take a look of the generated output.\"]'\n",
      "\n",
      "  \n",
      "================= New Header ================\n",
      "'markdown' cell: '['### Plot model flow graph\n",
      "Here, we visualize the model flow graph for the `ModelFlow`.']'\n",
      "\n",
      " 'code' cell: '[\"graph = Viz.to_digraph(output[0]['root'])\"]'\n",
      "\n",
      "  'code' cell: '[\"graph = Viz.to_digraph(output[1]['root'])\"]'\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "markdown_symbol = \"\"\"\\'markdown\\' cell: \\'\"\"\"\n",
    "code_symbol = \"\"\"\\'code\\' cell: \\'\"\"\"\n",
    "content_splited_by_header =  split_string_based_on_markdown_header(raw_content[0].page_content,\n",
    "                                               markdown_symbol=markdown_symbol,\n",
    "                                               code_symbol=code_symbol)\n",
    "for j in content_splited_by_header:\n",
    "    print(\"================= New Header ================\")\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Uniflow on the self-instruct dataset (with prompt)\n",
    "\n",
    "Now we can extract knowledge from the given jupyter notebook via Uniflow! First, we need to define a [GuidedPrompt](https://github.com/CambioML/uniflow/blob/main/uniflow/schema.py#L57), which includes a prompt and a list of examples for the LLM to do few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = GuidedPrompt(\n",
    "    instruction=\"If there is a code cell, generate one question given the markdown cell and its corresponding \\\n",
    "answer based on code cell and its output. If there is no code cell, generate one question and its corresponding \\\n",
    "answer based on context. Following the format of the examples below to include the same context, question, and \\\n",
    "answer in the response.\",\n",
    "    examples=[\n",
    "        Context(\n",
    "            context=\"\"\"'markdown' cell: '['### Use LLM to generate data in Uniflow.\n",
    "            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\n",
    "            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\"\"\",\n",
    "            question=\"How to use LLM to generate data in Uniflow\",\n",
    "            answer=\"We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e89b3",
   "metadata": {},
   "source": [
    "In this example, we will use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace the `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "Now we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones. \n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformOpenAIConfig(\n",
    "    guided_prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb79ac",
   "metadata": {},
   "source": [
    "Since each example in the `guided_prompt` are wrapped by `Context`, we need to apply the same format on our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:19<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "data = [Context(context=p) for p in content_splited_by_header if len(p) > 10]\n",
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output': [{'response': [{'context': '\\'markdown\\' cell: \\'[\\'# Notebook for ModelFlow \\n\\', \"In this example, we will show you how to generate question-answers (QAs) from give text strings using OpenAI\\'s models via uniflow\\'s [ModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L11).\\', \\'code\\' cell: \\'\\']\\'',\n",
       "      'question': 'What will be demonstrated in this notebook for ModelFlow?',\n",
       "      'answer': \"This notebook will demonstrate how to generate question-answers (QAs) from given text strings using OpenAI's models via uniflow's ModelFlow.\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134d7bac0>},\n",
       " {'output': [{'response': [{'question': 'What steps are needed before running the code?',\n",
       "      'answer': \"Before running the code, you will need to set up the 'uniflow' conda environment and obtain a valid [OpenAI API key]. After obtaining the key, it needs to be set as the environment variable OPENAI_API_KEY within a .env file in the root directory of the repository. More details can be found in the instruction at https://github.com/CambioML/uniflow/tree/main#api-keys\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134d7b8e0>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\",\n",
       "      'question': 'How to use LLM to generate data in Uniflow',\n",
       "      'answer': \"We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134d7bcd0>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\",\n",
       "      'question': 'How to use LLM to generate data in Uniflow',\n",
       "      'answer': \"To use LLM to generate data in Uniflow, we can use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers by creating a configuration object with 'config = Config(model_config=OpenAIModelConfig())' and initializing a client with 'client = Client(config)'.\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134da8310>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']\",\n",
       "      'question': 'How to use LLM to generate data in Uniflow',\n",
       "      'answer': \"We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134e3b970>},\n",
       " {'output': [{'response': [{'error': 'No code cell found'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134e3ad40>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['Next, for the given raw text strings `raw_context_input` above, we convert them to the `Context` class to be processed by `uniflow`.']'\\n\\n 'code' cell: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'\",\n",
       "      'question': 'How do we convert the raw text strings to the Context class for processing by uniflow?',\n",
       "      'answer': \"The raw text strings are converted to the Context class for processing by uniflow using the following code: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134da91e0>},\n",
       " {'output': [{'response': [{'context': '\\'markdown\\' cell: \\'[\\'### Use LLM to generate data\\nIn this example, we use the base `Config` defaults with the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) to generate questions and answers.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'config = TransformConfig(\\', \\'    flow_name=\"TransformOpenAIFlow\",\\', \\'    model_config=OpenAIModelConfig()\\', \\')\\', \\'client = Client(config)\\']\\'',\n",
       "      'question': 'What configuration is used to generate questions and answers using LLM?',\n",
       "      'answer': 'The TransformConfig with flow_name=\"TransformOpenAIFlow\" and model_config=OpenAIModelConfig() is used to generate questions and answers.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134da92a0>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\",\n",
       "      'question': 'How to use LLM to generate data in Uniflow',\n",
       "      'answer': \"We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134e38490>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['### Use LLM to generate data in Uniflow. In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\",\n",
       "      'question': 'How to use LLM to generate data in Uniflow',\n",
       "      'answer': \"We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134da9420>},\n",
       " {'output': [{'response': [{'context': \"'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\",\n",
       "      'question': 'How to use LLM to generate data in Uniflow',\n",
       "      'answer': \"We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.node.Node at 0x134da94e0>}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the output into pandas table\n",
    "\n",
    "The output is a bit messy, we can reconstructure it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d7a120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'markdown' cell: '['# Notebook for ModelFlow \\n', \"In this example, we will show you how to generate question-answers (QAs) from give text strings using OpenAI's models via uniflow's [ModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L11).', 'code' cell: '']'</td>\n",
       "      <td>What will be demonstrated in this notebook for ModelFlow?</td>\n",
       "      <td>This notebook will demonstrate how to generate question-answers (QAs) from given text strings using OpenAI's models via uniflow's ModelFlow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "      <td>How to use LLM to generate data in Uniflow</td>\n",
       "      <td>We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "      <td>How to use LLM to generate data in Uniflow</td>\n",
       "      <td>To use LLM to generate data in Uniflow, we can use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers by creating a configuration object with 'config = Config(model_config=OpenAIModelConfig())' and initializing a client with 'client = Client(config)'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']</td>\n",
       "      <td>How to use LLM to generate data in Uniflow</td>\n",
       "      <td>We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'markdown' cell: '['Next, for the given raw text strings `raw_context_input` above, we convert them to the `Context` class to be processed by `uniflow`.']'\\n\\n 'code' cell: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'</td>\n",
       "      <td>How do we convert the raw text strings to the Context class for processing by uniflow?</td>\n",
       "      <td>The raw text strings are converted to the Context class for processing by uniflow using the following code: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data\\nIn this example, we use the base `Config` defaults with the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) to generate questions and answers.']'\\n\\n 'code' cell: '['config = TransformConfig(', '    flow_name=\"TransformOpenAIFlow\",', '    model_config=OpenAIModelConfig()', ')', 'client = Client(config)']'</td>\n",
       "      <td>What configuration is used to generate questions and answers using LLM?</td>\n",
       "      <td>The TransformConfig with flow_name=\"TransformOpenAIFlow\" and model_config=OpenAIModelConfig() is used to generate questions and answers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "      <td>How to use LLM to generate data in Uniflow</td>\n",
       "      <td>We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data in Uniflow. In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "      <td>How to use LLM to generate data in Uniflow</td>\n",
       "      <td>We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "      <td>How to use LLM to generate data in Uniflow</td>\n",
       "      <td>We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                               context                                                                                question                                                                                                                                                                                                                                                                                              answer\n",
       "0                                                                                                                'markdown' cell: '['# Notebook for ModelFlow \\n', \"In this example, we will show you how to generate question-answers (QAs) from give text strings using OpenAI's models via uniflow's [ModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L11).', 'code' cell: '']'                               What will be demonstrated in this notebook for ModelFlow?                                                                                                                                                        This notebook will demonstrate how to generate question-answers (QAs) from given text strings using OpenAI's models via uniflow's ModelFlow.\n",
       "1                                                                                                         'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'                                              How to use LLM to generate data in Uniflow                                                                                                                We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\n",
       "2                                                                                                         'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'                                              How to use LLM to generate data in Uniflow  To use LLM to generate data in Uniflow, we can use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers by creating a configuration object with 'config = Config(model_config=OpenAIModelConfig())' and initializing a client with 'client = Client(config)'.\n",
       "3                                                                                                          'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']                                              How to use LLM to generate data in Uniflow                                                                                                                We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\n",
       "4                                                                                                                                                     'markdown' cell: '['Next, for the given raw text strings `raw_context_input` above, we convert them to the `Context` class to be processed by `uniflow`.']'\\n\\n 'code' cell: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'  How do we convert the raw text strings to the Context class for processing by uniflow?                                                                                                     The raw text strings are converted to the Context class for processing by uniflow using the following code: '['', 'data = [', '    Context(context=c)', '    for c in raw_context_input', ']']'\n",
       "5  'markdown' cell: '['### Use LLM to generate data\\nIn this example, we use the base `Config` defaults with the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) to generate questions and answers.']'\\n\\n 'code' cell: '['config = TransformConfig(', '    flow_name=\"TransformOpenAIFlow\",', '    model_config=OpenAIModelConfig()', ')', 'client = Client(config)']'                 What configuration is used to generate questions and answers using LLM?                                                                                                                                                            The TransformConfig with flow_name=\"TransformOpenAIFlow\" and model_config=OpenAIModelConfig() is used to generate questions and answers.\n",
       "6                                                                                                         'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'                                              How to use LLM to generate data in Uniflow                                                                                                                We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\n",
       "7                                                                                                                                  'markdown' cell: '['### Use LLM to generate data in Uniflow. In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'                                              How to use LLM to generate data in Uniflow                                                                                                                We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'\n",
       "8                                                                                                         'markdown' cell: '['### Use LLM to generate data in Uniflow.\\n            In this example, we use the base `Config` defaults with the [OpenAIModelConfig] to generate questions and answers.']'\\n            'code' cell: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'                                              How to use LLM to generate data in Uniflow                                                                                                                We can use the Uniflow's default [OpenAIModelConfig] to generate questions and answers with code: '['config = Config(model_config=OpenAIModelConfig())', 'client = Client(config)']'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "df = pd.DataFrame([{'context': response['context'],\n",
    "                    'question': response['question'],\n",
    "                    'answer': response['answer']}\n",
    "                   for item in output\n",
    "                   for i in item['output']\n",
    "                   for response in i['response'] if 'context' in response])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26797da6",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
